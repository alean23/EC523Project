{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running sqlite3 through python to query our sqlite database. Due to size of the database and constraints with running in this method we will break apart the query into multiple queries and create several csvs. After the csvs are created we will then use python to form the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 11 Home Players Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('database.sqlite')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for i in range(4, 12):  # Loop through numbers 1 to 11\n",
    "    # Dynamic view name and player references in the SQL query\n",
    "    create_view_query = f\"\"\"\n",
    "CREATE VIEW IF NOT EXISTS PlayerV9_{i} AS\n",
    "WITH MatchDates AS (\n",
    "    SELECT \n",
    "        m.id,\n",
    "        m.match_api_id,\n",
    "        m.date AS match_date,\n",
    "        m.home_player_{i},\n",
    "        m.home_team_api_id,\n",
    "        m.away_team_api_id\n",
    "    FROM Match m\n",
    "    WHERE m.league_id = 21518\n",
    ")\n",
    "SELECT \n",
    "    md.id,\n",
    "    md.match_api_id,\n",
    "    md.match_date,\n",
    "    md.home_team_api_id,\n",
    "    md.away_team_api_id,\n",
    "    p.player_name AS home_player_{i}_name,\n",
    "    (\n",
    "        SELECT pa.overall_rating\n",
    "        FROM Player_Attributes pa\n",
    "        WHERE pa.player_fifa_api_id = p.player_fifa_api_id\n",
    "        AND pa.date <= md.match_date\n",
    "        ORDER BY pa.date DESC\n",
    "        LIMIT 1\n",
    "    ) AS home_player_{i}_rating,\n",
    "    (\n",
    "        SELECT pa.potential\n",
    "        FROM Player_Attributes pa\n",
    "        WHERE pa.player_fifa_api_id = p.player_fifa_api_id\n",
    "        AND pa.date <= md.match_date\n",
    "        ORDER BY pa.date DESC\n",
    "        LIMIT 1\n",
    "    ) AS home_player_{i}_potential\n",
    "FROM \n",
    "    MatchDates md\n",
    "LEFT JOIN Player p ON md.home_player_{i} = p.player_api_id;\n",
    "\"\"\"\n",
    "\n",
    "    # Execute the SQL query to create or replace the view\n",
    "    cursor.execute(create_view_query)\n",
    "    conn.commit()  # Commit the view creation to the database\n",
    "\n",
    "    # Query the view to fetch data\n",
    "    cursor.execute(f\"SELECT * FROM PlayerV9_{i}\")\n",
    "\n",
    "    # Fetch all results\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Column headers based on the fetched data\n",
    "    headers = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Write data to a CSV file for the current player\n",
    "    with open(f'Home_Player_{i}.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)  # Write the headers\n",
    "        writer.writerows(rows)    # Write the data rows\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next 11 Away Players Independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('database.sqlite')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for i in range(10, 12):  # Loop through numbers 1 to 11\n",
    "    # Dynamic view name and player references in the SQL query\n",
    "    create_view_query = f\"\"\"\n",
    "CREATE VIEW IF NOT EXISTS PlayerV13_{i} AS\n",
    "WITH MatchDates AS (\n",
    "    SELECT \n",
    "        m.id,\n",
    "        m.match_api_id,\n",
    "        m.date AS match_date,\n",
    "        m.away_player_{i},\n",
    "        m.home_team_api_id,\n",
    "        m.away_team_api_id\n",
    "    FROM Match m\n",
    "    WHERE m.league_id = 21518\n",
    ")\n",
    "SELECT \n",
    "    md.id,\n",
    "    md.match_api_id,\n",
    "    md.match_date,\n",
    "    md.home_team_api_id,\n",
    "    md.away_team_api_id,\n",
    "    p.player_name AS away_player_{i}_name,\n",
    "    (\n",
    "        SELECT pa.overall_rating\n",
    "        FROM Player_Attributes pa\n",
    "        WHERE pa.player_fifa_api_id = p.player_fifa_api_id\n",
    "        AND pa.date <= md.match_date\n",
    "        ORDER BY pa.date DESC\n",
    "        LIMIT 1\n",
    "    ) AS away_player_{i}_rating,\n",
    "    (\n",
    "        SELECT pa.potential\n",
    "        FROM Player_Attributes pa\n",
    "        WHERE pa.player_fifa_api_id = p.player_fifa_api_id\n",
    "        AND pa.date <= md.match_date\n",
    "        ORDER BY pa.date DESC\n",
    "        LIMIT 1\n",
    "    ) AS away_player_{i}_potential\n",
    "FROM \n",
    "    MatchDates md\n",
    "LEFT JOIN Player p ON md.away_player_{i} = p.player_api_id;\n",
    "\"\"\"\n",
    "\n",
    "    # Execute the SQL query to create or replace the view\n",
    "    cursor.execute(create_view_query)\n",
    "    conn.commit()  # Commit the view creation to the database\n",
    "\n",
    "    # Query the view to fetch data\n",
    "    cursor.execute(f\"SELECT * FROM PlayerV13_{i}\")\n",
    "\n",
    "    # Fetch all results\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Column headers based on the fetched data\n",
    "    headers = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Write data to a CSV file for the current player\n",
    "    with open(f'Away_Player_{i}.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)  # Write the headers\n",
    "        writer.writerows(rows)    # Write the data rows\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all of the individual csvs are created I then want to combine them to start a new csv that contains all of the applicable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('database.sqlite')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for i in range(1, 12):  # Loop through numbers 1 to 11\n",
    "    # Dynamic view name and player references in the SQL query\n",
    "    create_view_query = f\"\"\"\n",
    "CREATE VIEW IF NOT EXISTS PlayerV6_{i} AS\n",
    "WITH MatchDates AS (\n",
    "    SELECT \n",
    "        m.id,\n",
    "        m.match_api_id,\n",
    "        m.date AS match_date,\n",
    "        m.home_player_{i},\n",
    "        m.home_team_api_id,\n",
    "        m.away_team_api_id\n",
    "    FROM Match m\n",
    "    WHERE m.home_team_api_id = 8633 OR m.away_team_api_id = 8633\n",
    ")\n",
    "SELECT \n",
    "    md.id,\n",
    "    md.match_api_id,\n",
    "    md.match_date,\n",
    "    p.player_name AS home_player_{i}_name,\n",
    "    (\n",
    "        SELECT pa.overall_rating\n",
    "        FROM Player_Attributes pa\n",
    "        WHERE pa.player_fifa_api_id = p.player_fifa_api_id\n",
    "        AND pa.date <= md.match_date\n",
    "        ORDER BY pa.date DESC\n",
    "        LIMIT 1\n",
    "    ) AS home_player_{i}_rating,\n",
    "    (\n",
    "        SELECT pa.potential\n",
    "        FROM Player_Attributes pa\n",
    "        WHERE pa.player_fifa_api_id = p.player_fifa_api_id\n",
    "        AND pa.date <= md.match_date\n",
    "        ORDER BY pa.date DESC\n",
    "        LIMIT 1\n",
    "    ) AS home_player_{i}_potential\n",
    "FROM \n",
    "    MatchDates md\n",
    "LEFT JOIN Player p ON md.home_player_{i} = p.player_api_id;\n",
    "\"\"\"\n",
    "\n",
    "    # Execute the SQL query to create or replace the view\n",
    "    cursor.execute(create_view_query)\n",
    "    conn.commit()  # Commit the view creation to the database\n",
    "\n",
    "    # Query the view to fetch data\n",
    "    cursor.execute(f\"SELECT * FROM PlayerV6_{i}\")\n",
    "\n",
    "    # Fetch all results\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Column headers based on the fetched data\n",
    "    headers = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Write data to a CSV file for the current player\n",
    "    with open(f'Home_Player_{i}.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)  # Write the headers\n",
    "        writer.writerows(rows)    # Write the data rows\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read and store each player's CSV file\n",
    "for i in range(1, 12):\n",
    "    df = pd.read_csv(f'Home_Player_{i}.csv')\n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all DataFrames with proper suffixes\n",
    "merged_df = dfs[0]  # Start with first DataFrame\n",
    "\n",
    "for i, df in enumerate(dfs[1:], start=2):\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        df,\n",
    "        on=['match_api_id', 'match_date'],\n",
    "        how='outer',\n",
    "        suffixes=(f'_{i-1}', f'_{i}')\n",
    "    )\n",
    "\n",
    "# Save the merged DataFrame\n",
    "merged_df.to_csv('all_players_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have all of the combined player ratings, potential, and names I now need to find the other information and add those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported 304 matches to matches_with_team_names.csv\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "def export_matches_with_team_names():\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect('database.sqlite')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # SQL query with team name joins\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        m.id,\n",
    "        m.country_id,\n",
    "        m.league_id,\n",
    "        m.season,\n",
    "        m.stage,\n",
    "        m.date,\n",
    "        m.match_api_id,\n",
    "        m.home_team_api_id,\n",
    "        home_team.team_long_name AS home_team_name,\n",
    "        m.away_team_api_id,\n",
    "        away_team.team_long_name AS away_team_name,\n",
    "        m.home_team_goal,\n",
    "        m.away_team_goal,\n",
    "        --m.goal,\n",
    "        --m.shoton,\n",
    "        --m.shotoff,\n",
    "        --m.foulcommit,\n",
    "        --m.card,\n",
    "       -- m.cross,\n",
    "        --m.corner,\n",
    "       -- m.possession,\n",
    "        m.B365H,\n",
    "        m.B365D, m.B365A, m.BWH, m.BWD, m.BWA, m.IWH, m.IWD, m.IWA,\n",
    "        m.LBH, m.LBD, m.LBA, m.PSH, m.PSD, m.PSA, m.WHH, m.WHD, m.WHA,\n",
    "        m.SJH, m.SJD, m.SJA, m.VCH, m.VCD, m.VCA, m.GBH, m.GBD, m.GBA,\n",
    "        m.BSH, m.BSD, m.BSA\n",
    "    FROM \n",
    "        Match AS m\n",
    "    LEFT JOIN Team AS home_team ON m.home_team_api_id = home_team.team_api_id\n",
    "    LEFT JOIN Team AS away_team ON m.away_team_api_id = away_team.team_api_id\n",
    "    where\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    # Get all rows and column names\n",
    "    rows = cursor.fetchall()\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    \n",
    "    # Write to CSV\n",
    "    csv_file = 'matches_with_team_names.csv'\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(column_names)  # Write header\n",
    "        writer.writerows(rows)  # Write all rows\n",
    "    \n",
    "    print(f\"Successfully exported {len(rows)} matches to {csv_file}\")\n",
    "    \n",
    "    # Close connection\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    export_matches_with_team_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trouble with the xml and unsure how neccesary it will be - for now will not use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to matches_with_player_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def parse_xml_columns(row):\n",
    "    \"\"\"\n",
    "    Parse all XML columns in a row and return aggregated player statistics\n",
    "    \"\"\"\n",
    "    player_stats = defaultdict(lambda: {\n",
    "        'goals': 0,\n",
    "        'assists': 0,\n",
    "        'yellow_cards': 0,\n",
    "        'red_cards': 0,\n",
    "        'shots_on': 0,\n",
    "        'shots_off': 0,\n",
    "        'fouls_committed': 0,\n",
    "        'crosses': 0,\n",
    "        'corners': 0\n",
    "    })\n",
    "    \n",
    "    xml_columns = ['goal', 'shoton', 'shotoff', 'foulcommit', 'card', 'cross', 'corner']\n",
    "    \n",
    "    for col in xml_columns:\n",
    "        xml_data = row.get(col, '')\n",
    "        if not xml_data or xml_data.lower() == 'nan':\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            root = ET.fromstring(f\"<root>{xml_data}</root>\")\n",
    "        except ET.ParseError:\n",
    "            continue\n",
    "            \n",
    "        for event in root:\n",
    "            for value in event.findall('value'):\n",
    "                # Common fields\n",
    "                player1 = value.find('player1')\n",
    "                player_id = player1.text if player1 is not None else None\n",
    "                team = value.find('team')\n",
    "                team_id = team.text if team is not None else None\n",
    "                \n",
    "                # Process each event type\n",
    "                if event.tag == 'goal':\n",
    "                    if player_id:\n",
    "                        player_stats[player_id]['goals'] += 1\n",
    "                    player2 = value.find('player2')\n",
    "                    if player2 is not None:\n",
    "                        player_stats[player2.text]['assists'] += 1\n",
    "                \n",
    "                elif event.tag == 'card':\n",
    "                    card_type = value.find('card_type')\n",
    "                    if card_type is not None and player_id:\n",
    "                        if card_type.text == 'y':\n",
    "                            player_stats[player_id]['yellow_cards'] += 1\n",
    "                        elif card_type.text == 'r':\n",
    "                            player_stats[player_id]['red_cards'] += 1\n",
    "                \n",
    "                elif event.tag == 'shoton' and player_id:\n",
    "                    player_stats[player_id]['shots_on'] += 1\n",
    "                \n",
    "                elif event.tag == 'shotoff' and player_id:\n",
    "                    player_stats[player_id]['shots_off'] += 1\n",
    "                \n",
    "                elif event.tag == 'foulcommit' and player_id:\n",
    "                    player_stats[player_id]['fouls_committed'] += 1\n",
    "                \n",
    "                elif event.tag == 'cross' and player_id:\n",
    "                    player_stats[player_id]['crosses'] += 1\n",
    "                \n",
    "                elif event.tag == 'corner' and player_id:\n",
    "                    player_stats[player_id]['corners'] += 1\n",
    "    \n",
    "    return dict(player_stats)\n",
    "\n",
    "def process_csv(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process the input CSV and create a new CSV with parsed statistics\n",
    "    \"\"\"\n",
    "    # Read the input CSV\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Prepare output data\n",
    "    output_rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Get the basic match info (non-XML columns)\n",
    "        match_info = {\n",
    "            'id': row['id'],\n",
    "            'match_api_id': row['match_api_id'],\n",
    "            'date': row['date'],\n",
    "            'home_team_api_id': row['home_team_api_id'],\n",
    "            'home_team_name': row.get('home_team_name', ''),\n",
    "            'away_team_api_id': row['away_team_api_id'],\n",
    "            'away_team_name': row.get('away_team_name', ''),\n",
    "            'home_team_goal': row['home_team_goal'],\n",
    "            'away_team_goal': row['away_team_goal']\n",
    "        }\n",
    "        \n",
    "        # Parse the XML columns\n",
    "        player_stats = parse_xml_columns(row)\n",
    "        \n",
    "        # Add player stats to match info\n",
    "        for player_id, stats in player_stats.items():\n",
    "            output_row = match_info.copy()\n",
    "            output_row.update({\n",
    "                'player_id': player_id,\n",
    "                **stats\n",
    "            })\n",
    "            output_rows.append(output_row)\n",
    "    \n",
    "    # Create DataFrame from output rows\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv = 'matches_with_team_names.csv'\n",
    "output_csv = 'matches_with_player_stats.csv'\n",
    "process_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to combine the two csv files so that I finally have a working dataset before I will then perform data augmentation to make the data more usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to matches_with_team_names.csv\n",
      "New columns: avg_home_prob, avg_draw_prob, avg_away_prob\n"
     ]
    }
   ],
   "source": [
    "## First want to reduce the betting columns into one average as we do not need to have for every sportsbook. \n",
    "import pandas as pd\n",
    "\n",
    "def odds_to_probability(odds):\n",
    "    \"\"\"Convert decimal odds to implied probability\"\"\"\n",
    "    return 1 / float(odds) if pd.notna(odds) else None\n",
    "\n",
    "def process_betting_odds(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Process betting odds and replace them with average probabilities.\n",
    "    If output_file is None, overwrites the input file.\n",
    "    \"\"\"\n",
    "    # Read the input CSV\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # List of all bookmaker columns (home, draw, away)\n",
    "    bookmakers = [\n",
    "        ('B365', ['B365H', 'B365D', 'B365A']),\n",
    "        ('BW', ['BWH', 'BWD', 'BWA']),\n",
    "        ('IW', ['IWH', 'IWD', 'IWA']),\n",
    "        ('LB', ['LBH', 'LBD', 'LBA']),\n",
    "        ('PS', ['PSH', 'PSD', 'PSA']),\n",
    "        ('WH', ['WHH', 'WHD', 'WHA']),\n",
    "        ('SJ', ['SJH', 'SJD', 'SJA']),\n",
    "        ('VC', ['VCH', 'VCD', 'VCA']),\n",
    "        ('GB', ['GBH', 'GBD', 'GBA']),\n",
    "        ('BS', ['BSH', 'BSD', 'BSA'])\n",
    "    ]\n",
    "    \n",
    "    # Initialize lists to collect all probabilities\n",
    "    all_home_probs = []\n",
    "    all_draw_probs = []\n",
    "    all_away_probs = []\n",
    "    \n",
    "    # Calculate probabilities for each bookmaker\n",
    "    for prefix, (h_col, d_col, a_col) in bookmakers:\n",
    "        # Convert odds to probabilities\n",
    "        home_probs = df[h_col].apply(odds_to_probability)\n",
    "        draw_probs = df[d_col].apply(odds_to_probability)\n",
    "        away_probs = df[a_col].apply(odds_to_probability)\n",
    "        \n",
    "        # Normalize to sum to 1 (account for bookmaker overround)\n",
    "        total_probs = home_probs + draw_probs + away_probs\n",
    "        home_probs = home_probs / total_probs\n",
    "        draw_probs = draw_probs / total_probs\n",
    "        away_probs = away_probs / total_probs\n",
    "        \n",
    "        # Collect probabilities for averaging\n",
    "        all_home_probs.append(home_probs)\n",
    "        all_draw_probs.append(draw_probs)\n",
    "        all_away_probs.append(away_probs)\n",
    "    \n",
    "    # Calculate average probabilities across all bookmakers\n",
    "    df['avg_home_prob'] = pd.concat(all_home_probs, axis=1).mean(axis=1)\n",
    "    df['avg_draw_prob'] = pd.concat(all_draw_probs, axis=1).mean(axis=1)\n",
    "    df['avg_away_prob'] = pd.concat(all_away_probs, axis=1).mean(axis=1)\n",
    "    \n",
    "    # Drop all the original betting columns\n",
    "    original_odds_cols = [col for _, cols in bookmakers for col in cols]\n",
    "    df.drop(columns=original_odds_cols, inplace=True)\n",
    "    \n",
    "    # Save to file (overwrite if no output_file specified)\n",
    "    save_path = output_file if output_file else input_file\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Processed data saved to {save_path}\")\n",
    "    print(\"New columns: avg_home_prob, avg_draw_prob, avg_away_prob\")\n",
    "\n",
    "# Example usage (overwrites original file):\n",
    "process_betting_odds('matches_with_team_names.csv')\n",
    "\n",
    "# Alternative usage (creates new file):\n",
    "# process_betting_odds('matches_with_team_names.csv', 'matches_with_probabilities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will create a marged output that is almost ready for use as a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to final_combined_data.csv\n",
      "Total rows: 304\n",
      "Columns: id, country_id, league_id, season, stage, date, match_api_id, home_team_api_id, home_team_name, away_team_api_id, away_team_name, home_team_goal, away_team_goal, avg_home_prob, avg_draw_prob, avg_away_prob, away_player_1_name, away_player_1_rating, away_player_1_potential, away_player_2_name, away_player_2_rating, away_player_2_potential, away_player_3_name, away_player_3_rating, away_player_3_potential, away_player_4_name, away_player_4_rating, away_player_4_potential, away_player_5_name, away_player_5_rating, away_player_5_potential, away_player_6_name, away_player_6_rating, away_player_6_potential, away_player_7_name, away_player_7_rating, away_player_7_potential, away_player_8_name, away_player_8_rating, away_player_8_potential, away_player_9_name, away_player_9_rating, away_player_9_potential, away_player_10_name, away_player_10_rating, away_player_10_potential, away_player_11_name, away_player_11_rating, away_player_11_potential, home_player_1_name, home_player_1_rating, home_player_1_potential, home_player_2_name, home_player_2_rating, home_player_2_potential, home_player_3_name, home_player_3_rating, home_player_3_potential, home_player_4_name, home_player_4_rating, home_player_4_potential, home_player_5_name, home_player_5_rating, home_player_5_potential, home_player_6_name, home_player_6_rating, home_player_6_potential, home_player_7_name, home_player_7_rating, home_player_7_potential, home_player_8_name, home_player_8_rating, home_player_8_potential, home_player_9_name, home_player_9_rating, home_player_9_potential, home_player_10_name, home_player_10_rating, home_player_10_potential, home_player_11_name, home_player_11_rating, home_player_11_potential\n"
     ]
    }
   ],
   "source": [
    "def combine_csv_files(matches_file, players_file, output_file):\n",
    "    \"\"\"\n",
    "    Combine matches_with_team_names.csv and combined_players.csv based on 'id'\n",
    "    \"\"\"\n",
    "    # Read both CSV files\n",
    "    matches_df = pd.read_csv(matches_file)\n",
    "    players_df = pd.read_csv(players_file)\n",
    "    \n",
    "    # Merge the dataframes on 'id'\n",
    "    combined_df = pd.merge(\n",
    "        matches_df,\n",
    "        players_df.drop(columns=['match_api_id', 'match_date']),  # Remove duplicate columns\n",
    "        on='id',\n",
    "        how='inner'  # Only keep rows that exist in both files\n",
    "    )\n",
    "    \n",
    "    # Reorder columns to have matches columns first\n",
    "    matches_columns = matches_df.columns.tolist()\n",
    "    players_columns = [col for col in players_df.columns \n",
    "                      if col not in ['id', 'match_api_id', 'match_date']]\n",
    "    \n",
    "    final_columns = matches_columns + players_columns\n",
    "    combined_df = combined_df[final_columns]\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined data saved to {output_file}\")\n",
    "    print(f\"Total rows: {len(combined_df)}\")\n",
    "    print(f\"Columns: {', '.join(combined_df.columns)}\")\n",
    "\n",
    "# Example usage\n",
    "combine_csv_files(\n",
    "    'matches_with_team_names.csv',\n",
    "    'combined_players.csv',\n",
    "    'final_combined_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to add record of each teeam before the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported 6080 matches to team_records.csv\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "conn = sqlite3.connect('database.sqlite')\n",
    "cursor = conn.cursor()\n",
    "query = f\"\"\"\n",
    "WITH MatchResults AS (\n",
    "    SELECT\n",
    "        match_api_id,\n",
    "        date,\n",
    "        season,\n",
    "        home_team_api_id AS team_id,\n",
    "        home_team_goal,\n",
    "        away_team_goal,\n",
    "        CASE\n",
    "            WHEN home_team_goal > away_team_goal THEN 'win'\n",
    "            WHEN home_team_goal = away_team_goal THEN 'draw'\n",
    "            ELSE 'loss'\n",
    "        END as result\n",
    "    FROM Match\n",
    "    WHERE league_id = 21518\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        match_api_id,\n",
    "        date,\n",
    "        season,\n",
    "        away_team_api_id AS team_id,\n",
    "        away_team_goal AS home_team_goal,\n",
    "        home_team_goal AS away_team_goal,\n",
    "        CASE\n",
    "            WHEN away_team_goal > home_team_goal THEN 'win'\n",
    "            WHEN away_team_goal = home_team_goal THEN 'draw'\n",
    "            ELSE 'loss'\n",
    "        END as result\n",
    "    FROM Match\n",
    "    WHERE league_id = 21518\n",
    "),\n",
    "CumulativeRecords AS (\n",
    "    SELECT\n",
    "        match_api_id,\n",
    "        date,\n",
    "        season,\n",
    "        team_id,\n",
    "        result,\n",
    "        SUM(CASE WHEN result = 'win' THEN 1 ELSE 0 END) OVER (PARTITION BY team_id, season ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) as wins,\n",
    "        SUM(CASE WHEN result = 'draw' THEN 1 ELSE 0 END) OVER (PARTITION BY team_id, season ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) as draws,\n",
    "        SUM(CASE WHEN result = 'loss' THEN 1 ELSE 0 END) OVER (PARTITION BY team_id, season ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) as losses\n",
    "    FROM MatchResults\n",
    ")\n",
    "SELECT match_api_id, date, season, team_id, wins, draws, losses\n",
    "FROM CumulativeRecords\n",
    "ORDER BY season, date, team_id;\n",
    "\"\"\"\n",
    "# Execute the query\n",
    "cursor.execute(query)\n",
    "    \n",
    "# Get all rows and column names\n",
    "rows = cursor.fetchall()\n",
    "column_names = [description[0] for description in cursor.description]\n",
    "                        \n",
    "# Write to CSV\n",
    "csv_file = 'team_records.csv'\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_names)  # Write header\n",
    "    writer.writerows(rows)  # Write all rows\n",
    "                        \n",
    "print(f\"Successfully exported {len(rows)} matches to {csv_file}\")\n",
    "                        \n",
    "                        # Close connection\n",
    "conn.close()                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to rearrange the data to make it more sensical for real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_player_info(row, prefix, player_num):\n",
    "    return [\n",
    "        row[f\"{prefix}_player_{player_num}_name\"],\n",
    "        row[f\"{prefix}_player_{player_num}_rating\"],\n",
    "        row[f\"{prefix}_player_{player_num}_potential\"]\n",
    "    ]\n",
    "\n",
    "def rearrange_row(row, real_api_id):\n",
    "    is_real_home = row['home_team_api_id'] == real_api_id\n",
    "    home_away_indicator = 1 if is_real_home else 0\n",
    "    team_prefix_real = 'home' if is_real_home else 'away'\n",
    "    team_prefix_other = 'away' if is_real_home else 'home'\n",
    "    \n",
    "    rearranged_data = [\n",
    "        home_away_indicator,\n",
    "        real_api_id,\n",
    "        row[f\"{team_prefix_other}_team_api_id\"],\n",
    "        row[f\"{team_prefix_other}_team_name\"],\n",
    "        row[f\"{team_prefix_real}_team_goal\"],\n",
    "        row[f\"{team_prefix_other}_team_goal\"],\n",
    "        row['avg_home_prob'] if is_real_home else row['avg_away_prob'],\n",
    "        row['avg_draw_prob'],\n",
    "        row['avg_away_prob'] if is_real_home else row['avg_home_prob']\n",
    "    ]\n",
    "    \n",
    "    # Append Real Madrid players' details\n",
    "    for i in range(1, 12):\n",
    "        rearranged_data.extend(get_player_info(row, team_prefix_real, i))\n",
    "    \n",
    "    # Append the other team's players' details\n",
    "    for i in range(1, 12):\n",
    "        rearranged_data.extend(get_player_info(row, team_prefix_other, i))\n",
    "    \n",
    "    return pd.Series(rearranged_data)\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('final_combined_data.csv')\n",
    "real_api_id = 8633  \n",
    "\n",
    "# Define new column names\n",
    "new_columns = [\n",
    "    'home_away_indicator', 'real_api_id', 'other_team_id', 'other_team_name',\n",
    "    'real_score', 'other_score', 'real_winprob', 'tie_prob', 'other_winprob'\n",
    "] + [f\"real_player_{i}_{attr}\" for i in range(1, 12) for attr in ['name', 'rating', 'potential']] \\\n",
    "  + [f\"other_player_{i}_{attr}\" for i in range(1, 12) for attr in ['name', 'rating', 'potential']]\n",
    "\n",
    "# Apply the function to each row\n",
    "rearranged_df = df.apply(lambda row: rearrange_row(row, real_api_id), axis=1)\n",
    "rearranged_df.columns = new_columns\n",
    "\n",
    "# Save the modified DataFrame\n",
    "rearranged_df.to_csv('rearranged_matches.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
